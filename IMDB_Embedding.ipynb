{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majid1363/majid1363/blob/main/IMDB_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFnurWg7MA9q"
      },
      "source": [
        "Let’s apply this idea to the IMDB movie-review sentiment-prediction task that\n",
        "you’re already familiar with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPW1PBLS6Yh1",
        "outputId": "aa6692bb-d2fb-4445-cb0b-bf1e3c68feac"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "# Number of words to consider as features\n",
        "max_features = 5000\n",
        "# Cuts off the text after this number of words (among the max_features most common words)\n",
        "max_len = 50\n",
        "\n",
        "# Loads the data as lists of integers\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Turns the lists of integers into a 2D integer tensor of shape (samples, max_len)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "#\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(25000, 50)\n",
            "(25000, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atATZlGO7mBt"
      },
      "source": [
        "The network will learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKTd4wYx7rkt",
        "outputId": "7d67f876-8101-4893-a636-df5ff6fc48b0"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dropout, Dense\n",
        "from keras import regularizers\n",
        "\n",
        "regularizer = regularizers.l2(l2=1e-5)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Specifies the maximum input length to the Embedding layer so you can later\n",
        "# flatten the embedded inputs. After the Embedding layer, the activations\n",
        "# have shape (samples, max_len, 8).\n",
        "model.add(Embedding(max_features, 8, input_length=max_len))\n",
        "\n",
        "# Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, max_len * 8)\n",
        "model.add(Flatten())\n",
        "\n",
        "# \n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Adds the classifier on top\n",
        "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizer))\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# fit model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 8)             40000     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 40,401\n",
            "Trainable params: 40,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6812 - acc: 0.5635 - val_loss: 0.5255 - val_acc: 0.7762\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4827 - acc: 0.7855 - val_loss: 0.4159 - val_acc: 0.8130\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3908 - acc: 0.8299 - val_loss: 0.3966 - val_acc: 0.8201\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3553 - acc: 0.8462 - val_loss: 0.3926 - val_acc: 0.8217\n",
            "Epoch 5/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3332 - acc: 0.8573 - val_loss: 0.3950 - val_acc: 0.8200\n",
            "Epoch 6/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3134 - acc: 0.8687 - val_loss: 0.3989 - val_acc: 0.8182\n",
            "Epoch 7/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3015 - acc: 0.8698 - val_loss: 0.4031 - val_acc: 0.8170\n",
            "Epoch 8/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2896 - acc: 0.8785 - val_loss: 0.4090 - val_acc: 0.8164\n",
            "Epoch 9/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2772 - acc: 0.8830 - val_loss: 0.4155 - val_acc: 0.8167\n",
            "Epoch 10/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2723 - acc: 0.8865 - val_loss: 0.4218 - val_acc: 0.8143\n",
            "Epoch 11/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2595 - acc: 0.8968 - val_loss: 0.4269 - val_acc: 0.8117\n",
            "Epoch 12/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2603 - acc: 0.8928 - val_loss: 0.4351 - val_acc: 0.8104\n",
            "Epoch 13/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2406 - acc: 0.9008 - val_loss: 0.4401 - val_acc: 0.8078\n",
            "Epoch 14/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2394 - acc: 0.9031 - val_loss: 0.4455 - val_acc: 0.8072\n",
            "Epoch 15/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2386 - acc: 0.9017 - val_loss: 0.4499 - val_acc: 0.8059\n",
            "Epoch 16/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2340 - acc: 0.9029 - val_loss: 0.4563 - val_acc: 0.8054\n",
            "Epoch 17/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2208 - acc: 0.9114 - val_loss: 0.4626 - val_acc: 0.8039\n",
            "Epoch 18/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2156 - acc: 0.9123 - val_loss: 0.4689 - val_acc: 0.8036\n",
            "Epoch 19/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2214 - acc: 0.9119 - val_loss: 0.4744 - val_acc: 0.8024\n",
            "Epoch 20/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2179 - acc: 0.9108 - val_loss: 0.4810 - val_acc: 0.8009\n",
            "Epoch 21/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2107 - acc: 0.9159 - val_loss: 0.4863 - val_acc: 0.8001\n",
            "Epoch 22/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2076 - acc: 0.9189 - val_loss: 0.4899 - val_acc: 0.8004\n",
            "Epoch 23/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2045 - acc: 0.9174 - val_loss: 0.4951 - val_acc: 0.7996\n",
            "Epoch 24/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2088 - acc: 0.9147 - val_loss: 0.4988 - val_acc: 0.7999\n",
            "Epoch 25/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1989 - acc: 0.9190 - val_loss: 0.5037 - val_acc: 0.7989\n",
            "Epoch 26/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1969 - acc: 0.9200 - val_loss: 0.5093 - val_acc: 0.7970\n",
            "Epoch 27/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1959 - acc: 0.9198 - val_loss: 0.5144 - val_acc: 0.7986\n",
            "Epoch 28/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1947 - acc: 0.9210 - val_loss: 0.5172 - val_acc: 0.7965\n",
            "Epoch 29/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1888 - acc: 0.9253 - val_loss: 0.5228 - val_acc: 0.7964\n",
            "Epoch 30/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1883 - acc: 0.9258 - val_loss: 0.5251 - val_acc: 0.7959\n",
            "Epoch 31/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1865 - acc: 0.9244 - val_loss: 0.5295 - val_acc: 0.7956\n",
            "Epoch 32/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1885 - acc: 0.9238 - val_loss: 0.5340 - val_acc: 0.7937\n",
            "Epoch 33/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1889 - acc: 0.9229 - val_loss: 0.5371 - val_acc: 0.7940\n",
            "Epoch 34/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1817 - acc: 0.9277 - val_loss: 0.5398 - val_acc: 0.7955\n",
            "Epoch 35/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1838 - acc: 0.9264 - val_loss: 0.5457 - val_acc: 0.7950\n",
            "Epoch 36/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1818 - acc: 0.9253 - val_loss: 0.5472 - val_acc: 0.7938\n",
            "Epoch 37/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1809 - acc: 0.9293 - val_loss: 0.5500 - val_acc: 0.7933\n",
            "Epoch 38/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1762 - acc: 0.9305 - val_loss: 0.5520 - val_acc: 0.7938\n",
            "Epoch 39/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1813 - acc: 0.9253 - val_loss: 0.5560 - val_acc: 0.7935\n",
            "Epoch 40/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1770 - acc: 0.9301 - val_loss: 0.5605 - val_acc: 0.7908\n",
            "Epoch 41/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1757 - acc: 0.9263 - val_loss: 0.5606 - val_acc: 0.7907\n",
            "Epoch 42/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1726 - acc: 0.9325 - val_loss: 0.5659 - val_acc: 0.7920\n",
            "Epoch 43/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1802 - acc: 0.9291 - val_loss: 0.5654 - val_acc: 0.7912\n",
            "Epoch 44/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1774 - acc: 0.9271 - val_loss: 0.5667 - val_acc: 0.7922\n",
            "Epoch 45/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1827 - acc: 0.9246 - val_loss: 0.5716 - val_acc: 0.7911\n",
            "Epoch 46/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1717 - acc: 0.9291 - val_loss: 0.5743 - val_acc: 0.7903\n",
            "Epoch 47/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1769 - acc: 0.9273 - val_loss: 0.5769 - val_acc: 0.7904\n",
            "Epoch 48/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1741 - acc: 0.9309 - val_loss: 0.5790 - val_acc: 0.7895\n",
            "Epoch 49/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1710 - acc: 0.9306 - val_loss: 0.5801 - val_acc: 0.7911\n",
            "Epoch 50/50\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.1716 - acc: 0.9302 - val_loss: 0.5831 - val_acc: 0.7896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoEfYR428cnM"
      },
      "source": [
        "You get to a validation accuracy of ~80%, which is pretty good considering that you’re only looking at the first 40 words in every review. But note that merely flattening the embedded sequences and training a single Dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word relationships and sentence structure (for example, this model would likely treat both “this movie is a bomb” and “this movie is the bomb” as being negative reviews). It’s much better to add recurrent layers or 1D convolutional layers on top of the embedded sequences to learn features that take into account each sequence as a whole. That’s what we’ll focus on in the next few sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch2upmxD8oor",
        "outputId": "e07381f8-ef69-4f13-cf12-0f8b96ce02f0"
      },
      "source": [
        "# \n",
        "weights = model.layers[0].get_weights()[0]\n",
        "print(weights.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}